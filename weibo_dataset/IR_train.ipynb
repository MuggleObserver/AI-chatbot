{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "SRC_TRAIN_DATA = \"index20wtrain.post\"  # 问题输入语言\n",
    "TRG_TRAIN_DATA = \"index20wtrain.resp\"  # 答案输入语言\n",
    "SRC_TRAIN_DATA_NEG = \"index20wtrainwcy.post\"  # 问题输入语言\n",
    "TRG_TRAIN_DATA_NEG = \"index20wtrainwcy.resp\"  # 答案输入语言\n",
    "\n",
    "CHECKPOINT_PATH = \"./IR_ckpt\"\n",
    "\n",
    "HIDDEN_SIZE = 300  # LSTM的隐藏层规模\n",
    "NUM_LAYERS = 2  # 深层循环神经网络中的LSTM结构的层数\n",
    "VOCAB_SIZE = 10000  # 词汇表的大小（词汇表按照词频，由高到低向下排列）\n",
    "BATCH_SIZE = 100  # 训练数据batch的大小\n",
    "NUM_EPOCH = 2  # 使用训练数据的轮数\n",
    "KEEP_PROB = 0.8  # 节点不被dropout的概率\n",
    "MAX_GRAD_NORM = 5  # 用于控制梯度膨胀的梯度大小上限\n",
    "BASE_LEARNING_RATE = 0.1  # 初始的学习率\n",
    "LEARNING_RATE_DECAY = 0.99  # 学习衰减率\n",
    "\n",
    "MAX_LEN = 50  # 限定句子的最大单词数量。\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "\n",
    "def getminbed():\n",
    "    f = open('minbed', 'r')\n",
    "    embedict = {}\n",
    "    rawlist = []\n",
    "    wordindex = {}\n",
    "    for i in range(9863):\n",
    "        p = f.readline()\n",
    "        p = p.strip()\n",
    "        list = p.split()\n",
    "        key = list.pop(0)\n",
    "        list2 = []\n",
    "        for t in list:\n",
    "            list2.append(float(t))\n",
    "        embedict.update({key: list2})\n",
    "        rawlist.append(list2)\n",
    "    f.close()\n",
    "    embedlist = np.array(rawlist)\n",
    "    wordindex = {}\n",
    "    i = 0\n",
    "    for key in embedict.keys():\n",
    "        wordindex[key] = i\n",
    "        i = i + 1\n",
    "    return rawlist\n",
    "\n",
    "\n",
    "# 定义IRModel类来描述模型\n",
    "class IRModel(object):\n",
    "    # 在模型的初始化函数中定义模型要用到的变量\n",
    "    def __init__(self):\n",
    "        # 定义编码器的LSTM结构\n",
    "        self.enc_cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [tf.nn.rnn_cell.LSTMCell(HIDDEN_SIZE)\n",
    "             for _ in range(NUM_LAYERS)])\n",
    "\n",
    "        init = tf.constant_initializer(getminbed())\n",
    "\n",
    "        # 为源语言和目标语言分别定义词向量\n",
    "        self.embedding = tf.get_variable(\"src_emb\", [VOCAB_SIZE, HIDDEN_SIZE], initializer=init, trainable=False)\n",
    "        self.W = tf.get_variable(\"weights\", [HIDDEN_SIZE, HIDDEN_SIZE],\n",
    "                                 initializer=tf.truncated_normal_initializer())\n",
    "\n",
    "    # 在forward函数中定义模型的前向计算图。\n",
    "    # src_input,src_size,trg_input,trg_size,targets分别是上面MakeSrcTrgDataset函数产生的五种张量。\n",
    "    def forward(self, src_input, src_size, trg_input, trg_size, targets):\n",
    "        batch_size = tf.shape(src_input)[0]\n",
    "        # src_emb和trg_emb的维度均为batch_size * max_time * HIDDEN_SIZE\n",
    "        src_emb = tf.nn.embedding_lookup(self.embedding, src_input)\n",
    "        trg_emb = tf.nn.embedding_lookup(self.embedding, trg_input)\n",
    "\n",
    "        # 使用dynamic_rnn构造编码器。\n",
    "        with tf.variable_scope(\"encoder\"):\n",
    "            enc_src_outputs, enc_src_state = tf.nn.dynamic_rnn(self.enc_cell, src_emb, sequence_length=src_size,\n",
    "                                                               dtype=tf.float32)\n",
    "            enc_trg_outputs, enc_trg_state = tf.nn.dynamic_rnn(self.enc_cell, trg_emb, sequence_length=trg_size,\n",
    "                                                               dtype=tf.float32)\n",
    "            # 因为编码器是一个双层LSTM，因此enc_state是一个包含两个LSTMStateTuple类的tuple，enc_state存储的是每一层的最后一个step的输出\n",
    "            # enc_src_outputs存储的是顶层LSTM的每一步输出，它的维度为[batch_size,max_time,HIDDEN_LAYER]\n",
    "            # src_enc_output, trg_enc_output的shape为BATCH_SIZE * HIDDEN_SIZE\n",
    "            src_enc_output = enc_src_outputs[:, -1, :]\n",
    "            trg_enc_output = enc_trg_outputs[:, -1, :]\n",
    "        # 通过编码生成的src_enc_output与W相乘，生成可能的回复generate_trg。\n",
    "        # 通过点乘的方式来预测生成的回复generate_trg和候选的回复trg_enc_output之间的相似程度，\n",
    "        # 点乘结果越大表示候选回复作为回复的可信度越高；\n",
    "        # 之后通过sigmoid函数归一化，转成概率形式。\n",
    "        generate_trg = tf.matmul(src_enc_output, self.W)\n",
    "        logits = tf.reduce_sum(tf.multiply(generate_trg, trg_enc_output), axis=1, keepdims=True)\n",
    "        probs = tf.sigmoid(logits)\n",
    "\n",
    "        # 计算 the binary cross-entropy loss\n",
    "        targets = tf.expand_dims(targets, 1)\n",
    "        losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=probs, labels=tf.to_float(targets))\n",
    "        # Mean loss across the batch of examples\n",
    "        mean_loss = tf.reduce_mean(losses, name=\"mean_loss\")\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            BASE_LEARNING_RATE,\n",
    "            global_step=global_step,\n",
    "            decay_steps=100,\n",
    "            decay_rate=LEARNING_RATE_DECAY\n",
    "        )\n",
    "        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(mean_loss, global_step=global_step)\n",
    "        return mean_loss, train_step\n",
    "\n",
    "\n",
    "def MakeDataset(file_path):\n",
    "    dataset = tf.data.TextLineDataset(file_path)\n",
    "    # 根据空格将单词编号切分开并放入一个一维向量。\n",
    "    dataset = dataset.map(lambda string: tf.string_split([string]).values)\n",
    "    # 将字符串形式的单词编号转化为整数。\n",
    "    dataset = dataset.map(\n",
    "        lambda string: tf.string_to_number(string, tf.int32))\n",
    "    # 统计每个句子的单词数量，并与句子内容一起放入Dataset中。\n",
    "    dataset = dataset.map(lambda x: (x, tf.size(x)))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def MakeSrcTrgDataset(src_path, trg_path, src_path_neg, trg_path_neg, batch_size):\n",
    "    # 统计src_path所含的总行数（src_path与trg_path所含行数相等）\n",
    "    def countLine(filepath):\n",
    "        count = 0\n",
    "        for index, line in enumerate(open(filepath, 'r')):\n",
    "            count += 1\n",
    "        return count\n",
    "\n",
    "    line_num = countLine(src_path)\n",
    "\n",
    "    src_data = MakeDataset(src_path)\n",
    "    trg_data = MakeDataset(trg_path)\n",
    "    src_data_neg = MakeDataset(src_path_neg)\n",
    "    trg_data_neg = MakeDataset(trg_path_neg)\n",
    "\n",
    "    data_label_1 = tf.data.Dataset.from_tensor_slices(tf.ones([line_num], dtype=tf.int32))\n",
    "    data_label_0 = tf.data.Dataset.from_tensor_slices(tf.zeros([line_num], dtype=tf.int32))\n",
    "\n",
    "    # 通过zip操作将两个Dataset合并为一个Dataset。现在每个Dataset中每一项数据ds\n",
    "    # 由4个张量组成：\n",
    "    #   ds[0][0]是源句子\n",
    "    #   ds[0][1]是源句子长度\n",
    "    #   ds[1][0]是目标句子\n",
    "    #   ds[1][1]是目标句子长度\n",
    "    #   ds[2][0]是标签\n",
    "    dataset_1 = tf.data.Dataset.zip((src_data, trg_data, data_label_1))\n",
    "    dataset_0 = tf.data.Dataset.zip((src_data_neg, trg_data_neg, data_label_0))\n",
    "    dataset = dataset_1.concatenate(dataset_0)\n",
    "\n",
    "    # 删除内容为空（只包含<EOS>）的句子和长度过长的句子。\n",
    "    def FilterLength(src_tuple, trg_tuple, label):\n",
    "        ((src_input, src_len), (trg_label, trg_len), label) = (src_tuple, trg_tuple, label)\n",
    "        src_len_ok = tf.logical_and(\n",
    "            tf.greater(src_len, 1), tf.less_equal(src_len, MAX_LEN))\n",
    "        trg_len_ok = tf.logical_and(\n",
    "            tf.greater(trg_len, 1), tf.less_equal(trg_len, MAX_LEN))\n",
    "        return tf.logical_and(src_len_ok, trg_len_ok)\n",
    "\n",
    "    dataset = dataset.filter(FilterLength)\n",
    "\n",
    "    # 随机打乱训练数据。\n",
    "    dataset = dataset.shuffle(400000)\n",
    "\n",
    "    # 规定填充后输出的数据维度。\n",
    "    padded_shapes = (\n",
    "        (tf.TensorShape([None]),  # 源句子是长度未知的向量\n",
    "         tf.TensorShape([])),  # 源句子长度是单个数字\n",
    "        (tf.TensorShape([None]),  # 目标句子是长度未知的向量\n",
    "         tf.TensorShape([])),  # 目标句子长度是单个数字\n",
    "        tf.TensorShape([]))  # 标签是单个数字\n",
    "    # 调用padded_batch方法进行batching操作。\n",
    "    batched_dataset = dataset.padded_batch(batch_size, padded_shapes)\n",
    "    return batched_dataset\n",
    "\n",
    "\n",
    "def run_epoch(session, cost_op, train_op, saver, step):\n",
    "    # 训练一个epoch\n",
    "    # 重复训练步骤，直至遍历完Dataset中所有的数据\n",
    "    while True:\n",
    "        try:\n",
    "            # 运行train_op，并计算损失值。训练数据在main()函数中以Dataset方式提供。\n",
    "            cost, _ = session.run([cost_op, train_op])\n",
    "            if step % 10 == 0:\n",
    "                print(\"After %d steps, per token cost is %.3f\" % (step, cost))\n",
    "            # 每200步保存一个checkpoint。\n",
    "            if step % 200 == 0:\n",
    "                saver.save(session, CHECKPOINT_PATH, global_step=step)\n",
    "            step += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    return step\n",
    "\n",
    "\n",
    "def main():\n",
    "    initializer = tf.random_uniform_initializer(- 0.25, 0.25)\n",
    "\n",
    "    with tf.variable_scope(\"IRModel\", reuse=None, initializer=initializer):\n",
    "        train_model = IRModel()\n",
    "    # 定义输入数据。\n",
    "    data = MakeSrcTrgDataset(SRC_TRAIN_DATA, TRG_TRAIN_DATA, SRC_TRAIN_DATA_NEG, TRG_TRAIN_DATA_NEG, BATCH_SIZE)\n",
    "    iterator = data.make_initializable_iterator()\n",
    "    (src, src_size), (trg_input, trg_size), label = iterator.get_next()\n",
    "    # 定义前向计算图。输入数据以张量形式提供给forward函数。\n",
    "    cost_op, train_op = train_model.forward(src, src_size, trg_input, trg_size, label)\n",
    "\n",
    "    # 训练模型。\n",
    "    saver = tf.train.Saver()\n",
    "    step = 0\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for i in range(NUM_EPOCH):\n",
    "            print(\"In iteration: %d\" % (i + 1))\n",
    "            sess.run(iterator.initializer)\n",
    "            step = run_epoch(sess, cost_op, train_op, saver, step)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
